## Neural Networks

- softplus

    - 

- relu

    - rectified linear unit

- activation functions

    - when building a nn you have to decide which activation function to use
    
    - in practice, the relu or softplus AF are more commonly used

- hidden layers

    - layers of nodes between the input and output nodes

    - usually decided by just guessing and seeing how the neural network performs

- 
